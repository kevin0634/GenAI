{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4HZ1FpTb6McjQmQav8Tlw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevin0634/GenAI/blob/main/Gimini_API_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B6EBtRpedsec"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "aZfqWaaFeTQS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"how to build a RAG pipeline using Llama LLM?\")"
      ],
      "metadata": {
        "id": "r0eRP4RreuWS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoxlYP_Xe_1n",
        "outputId": "f07a5a14-9eca-4f33-f1c7-d04d2b0a8530"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Building a RAG Pipeline using Llama LLM**\n",
            "\n",
            "**Step 1: Install Necessary Packages**\n",
            "\n",
            "```\n",
            "pip install datasets transformers texthero\n",
            "```\n",
            "\n",
            "**Step 2: Load the Llama LLM Model**\n",
            "\n",
            "```\n",
            "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"lama-llm\")\n",
            "model = AutoModelForSeq2SeqLM.from_pretrained(\"lama-llm\")\n",
            "```\n",
            "\n",
            "**Step 3: Prepare the Retrieval Dataset**\n",
            "\n",
            "```python\n",
            "import datasets\n",
            "import texthero as th\n",
            "\n",
            "# Load the collection of documents for retrieval\n",
            "dataset = datasets.load_dataset(\"wiki_dpr\")\n",
            "\n",
            "# Preprocess the text documents\n",
            "processed_dataset = dataset.map(\n",
            "    lambda example: {\"text\": th.clean(example[\"text\"])}, \n",
            "    batched=True, batch_size=16)\n",
            "```\n",
            "\n",
            "**Step 4: Create the Vector Encoder**\n",
            "\n",
            "```python\n",
            "# Convert the preprocessed documents into embeddings\n",
            "processed_dataset = processed_dataset.map(\n",
            "    lambda example: {\"embedding\": model.get_input_embeddings()(tokenizer(example[\"text\"]).input_ids).mean(dim=1)}, \n",
            "    batched=True, batch_size=16)\n",
            "```\n",
            "\n",
            "**Step 5: Generate Retrieval Candidates**\n",
            "\n",
            "```python\n",
            "import faiss\n",
            "\n",
            "# Create a faiss index for efficient nearest neighbor search\n",
            "index = faiss.IndexFlatL2(processed_dataset[\"embedding\"][0].shape[-1])\n",
            "index.add(processed_dataset[\"embedding\"][0])\n",
            "\n",
            "# Process the query and retrieve candidates\n",
            "query = \"Your Query\"\n",
            "query_embedding = model.get_input_embeddings()(tokenizer(query).input_ids).mean(dim=1)\n",
            "scores, indices = index.search(query_embedding, 10)\n",
            "candidate_texts = [processed_dataset[\"text\"][i] for i in indices[0]]\n",
            "```\n",
            "\n",
            "**Step 6: Generate Response**\n",
            "\n",
            "```python\n",
            "# Generate a response using the retrieved candidates as context\n",
            "response = model.generate(\n",
            "    tokenizer(query, return_tensors=\"pt\").input_ids,\n",
            "    max_length=128, \n",
            "    num_beams=4, \n",
            "    context=tokenizer(candidate_texts, return_tensors=\"pt\").input_ids)\n",
            "\n",
            "# Decode the generated response\n",
            "generated_text = tokenizer.batch_decode(response, skip_special_tokens=True)\n",
            "```\n",
            "\n",
            "**Step 7: Evaluate the Pipeline**\n",
            "\n",
            "```python\n",
            "# Metrics for evaluating the retrieval and generation components\n",
            "from datasets import load_metric\n",
            "\n",
            "retrieval_metric = load_metric(\"accuracy\")\n",
            "generation_metric = load_metric(\"rouge\")\n",
            "\n",
            "# Evaluate the retrieval component\n",
            "candidate_labels = [1 if i in indices[0] else 0 for i in range(len(dataset))]\n",
            "retrieval_metric.add_batch(predictions=indices[0], references=candidate_labels)\n",
            "\n",
            "# Evaluate the generation component\n",
            "generation_metric.add_batch(predictions=generated_text, references=dataset[\"answer\"][0])\n",
            "\n",
            "print(retrieval_metric.compute(), generation_metric.compute())\n",
            "```\n",
            "\n",
            "**Additional Considerations:**\n",
            "\n",
            "* You can fine-tune the Llama LLM for better retrieval and generation performance.\n",
            "* Experiment with different query expansion techniques to improve retrieval recall.\n",
            "* Optimize the faiss index parameters for faster search.\n",
            "* Consider using a more comprehensive retrieval dataset.\n"
          ]
        }
      ]
    }
  ]
}